{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "047e29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, pipeline\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b1d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para ajudar na análise dos dados\n",
    "from sklearn.metrics import (\n",
    "                    classification_report, \n",
    "                    confusion_matrix,\n",
    "                    roc_curve,\n",
    "                    auc,\n",
    "                    accuracy_score,\n",
    "                    precision_score,\n",
    "                    recall_score,\n",
    "                    f1_score,\n",
    "                    matthews_corrcoef, #PHI\n",
    "                    roc_auc_score,\n",
    "                    brier_score_loss,\n",
    "                    mean_squared_error,\n",
    "                    mean_absolute_error,\n",
    "                    median_absolute_error\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "be7b8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileResultPah = \"C:\\\\Users\\\\limajos\\\\Dropbox\\\\clima_tech_artigo\\\\resultado_gpt_model5.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bd4268ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 'TN'+'\\t'+'TP'+'\\t'+'FN'+'\\t'+'FP'+'\\t'+'0'+'\\t'+'1'+'\\t'+'accuracy'+'\\t'+'precision_micro'+'\\t'+'precision_macro'+'\\t'+'precision_weighted'+'\\t'+'precision_binary'+'\\t'+'recall_micro'+'\\t'+'recall_macro'+'\\t'+'recall_weighted'+'\\t'+'recall_binary'+'\\t'+'f1Score_micro'+'\\t'+'f1Score_macro'+'\\t'+'f1Score_weighted'+'\\t'+'f1Score_binary'+'\\t'+'phi'+'\\t'+'meanSquared_error'+'\\t'+'meanAbsolute_error'+'\\t'+'medianAbsolute_error'+'\\n'\n",
    "\n",
    "#Apaga o arquivo anterior que tenha o mesmo nome\n",
    "f = open(fileResultPah, \"w\")\n",
    "f.write(result)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "890cf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_mostrar_resultado(y_true, y_pred):\n",
    "    \n",
    "    classificationReport = classification_report(y_true, y_pred)\n",
    "    \n",
    "    classificationReportSplit = classificationReport.split() \n",
    "    \n",
    "    confusionMatrix = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "    precision_binary = precision_score(y_true, y_pred, average='binary')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "    recall_binary = recall_score(y_true, y_pred, average='binary')\n",
    "    f1Score_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1Score_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1Score_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    f1Score_binary = f1_score(y_true, y_pred, average='binary')\n",
    "    phi = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    meanSquared_error = mean_squared_error(y_true, y_pred)\n",
    "    meanAbsolute_error = mean_absolute_error(y_true, y_pred)\n",
    "    medianAbsolute_error = median_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    matrixConfusao = str(confusionMatrix[0][0])+'\\t'+str(confusionMatrix[1][1])+'\\t'+str(confusionMatrix[1][0])+'\\t'+str(confusionMatrix[0][1])\n",
    "    precisao = str(precision_micro)+'\\t'+str(precision_macro)+'\\t'+str(precision_weighted)+'\\t'+str(precision_binary)\n",
    "    recall = str(recall_micro)+'\\t'+str(recall_macro)+'\\t'+str(recall_weighted)+'\\t'+str(recall_binary)\n",
    "    f1Score = str(f1Score_micro)+'\\t'+str(f1Score_macro)+'\\t'+str(f1Score_weighted)+'\\t'+str(f1Score_binary)\n",
    "    \n",
    "\n",
    "    result = matrixConfusao+'\\t'+str(classificationReportSplit[5])+'\\t'+str(classificationReportSplit[10])+'\\t'+str(accuracy)+'\\t'+precisao+'\\t'+recall+'\\t'+f1Score+'\\t'+str(phi)+'\\t'+str(meanSquared_error)+'\\t'+str(meanAbsolute_error)+'\\t'+str(medianAbsolute_error)+'\\t'+str(y_true)+'\\t'+str(y_pred)+'\\n'\n",
    "    \n",
    "    result = result.replace('.',',')\n",
    "\n",
    "    f = open(fileResultPah, \"a\")\n",
    "    f.write(result)\n",
    "    f.close()\n",
    "\n",
    "    print(result)  \n",
    "    print(confusionMatrix)  \n",
    "    print(classificationReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61fcf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGHT = 512\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.2 \n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92407b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./dataset_concat_v3_.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9480a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a49303b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49739</th>\n",
       "      <td>50033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutamente perfeito, sem falhas!</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49740</th>\n",
       "      <td>50035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experiência excelente, sem nenhum defeito!</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49741</th>\n",
       "      <td>50037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sem problemas detectados!</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49742</th>\n",
       "      <td>50041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nada a reclamar, tudo perfeito!</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49743</th>\n",
       "      <td>50053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nenhum problema, sem falhas!</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49744 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            text_en  \\\n",
       "0          1  Once again Mr. Costner has dragged out a movie...   \n",
       "1          2  This is an example of why the majority of acti...   \n",
       "2          3  First of all I hate those moronic rappers, who...   \n",
       "3          4  Not even the Beatles could write songs everyon...   \n",
       "4          5  Brass pictures movies is not a fitting word fo...   \n",
       "...      ...                                                ...   \n",
       "49739  50033                                                NaN   \n",
       "49740  50035                                                NaN   \n",
       "49741  50037                                                NaN   \n",
       "49742  50041                                                NaN   \n",
       "49743  50053                                                NaN   \n",
       "\n",
       "                                                 text_pt sentiment  \n",
       "0      Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1      Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2      Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3      Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "4      Filmes de fotos de latão não é uma palavra apr...       neg  \n",
       "...                                                  ...       ...  \n",
       "49739                Absolutamente perfeito, sem falhas!  positivo  \n",
       "49740         Experiência excelente, sem nenhum defeito!  positivo  \n",
       "49741                          Sem problemas detectados!  positivo  \n",
       "49742                    Nada a reclamar, tudo perfeito!  positivo  \n",
       "49743                       Nenhum problema, sem falhas!  positivo  \n",
       "\n",
       "[49744 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc58458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neg    24763\n",
       "pos    24678\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de valores válidos para a coluna 'sentiment'\n",
    "valores_validos = ['pos', 'neg']\n",
    "#valores_validos = ['pos', 'neg', 'positivo', 'negativo']\n",
    "\n",
    "# Filtrar o DataFrame mantendo apenas as linhas com valores válidos na coluna 'sentiment'\n",
    "df_filtrado = df[df['sentiment'].isin(valores_validos)]\n",
    "\n",
    "df = df_filtrado\n",
    "\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53a34d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49436</th>\n",
       "      <td>49456</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>Como a média de votos era muito baixa, e o fat...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49437</th>\n",
       "      <td>49457</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>O enredo teve algumas reviravoltas infelizes e...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49438</th>\n",
       "      <td>49458</td>\n",
       "      <td>I am amazed at how this movieand most others h...</td>\n",
       "      <td>Estou espantado com a forma como este filme e ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49439</th>\n",
       "      <td>49459</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>A Christmas Together realmente veio antes do m...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49440</th>\n",
       "      <td>49460</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>O drama romântico da classe trabalhadora do di...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49441 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            text_en  \\\n",
       "0          1  Once again Mr. Costner has dragged out a movie...   \n",
       "1          2  This is an example of why the majority of acti...   \n",
       "2          3  First of all I hate those moronic rappers, who...   \n",
       "3          4  Not even the Beatles could write songs everyon...   \n",
       "4          5  Brass pictures movies is not a fitting word fo...   \n",
       "...      ...                                                ...   \n",
       "49436  49456  Seeing as the vote average was pretty low, and...   \n",
       "49437  49457  The plot had some wretched, unbelievable twist...   \n",
       "49438  49458  I am amazed at how this movieand most others h...   \n",
       "49439  49459  A Christmas Together actually came before my t...   \n",
       "49440  49460  Working-class romantic drama from director Mar...   \n",
       "\n",
       "                                                 text_pt sentiment  \n",
       "0      Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1      Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2      Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3      Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "4      Filmes de fotos de latão não é uma palavra apr...       neg  \n",
       "...                                                  ...       ...  \n",
       "49436  Como a média de votos era muito baixa, e o fat...       pos  \n",
       "49437  O enredo teve algumas reviravoltas infelizes e...       pos  \n",
       "49438  Estou espantado com a forma como este filme e ...       pos  \n",
       "49439  A Christmas Together realmente veio antes do m...       pos  \n",
       "49440  O drama romântico da classe trabalhadora do di...       pos  \n",
       "\n",
       "[49441 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9226bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            text_en  \\\n",
       "0   1  Once again Mr. Costner has dragged out a movie...   \n",
       "1   2  This is an example of why the majority of acti...   \n",
       "2   3  First of all I hate those moronic rappers, who...   \n",
       "3   4  Not even the Beatles could write songs everyon...   \n",
       "4   5  Brass pictures movies is not a fitting word fo...   \n",
       "\n",
       "                                             text_pt sentiment  \n",
       "0  Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1  Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2  Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3  Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "4  Filmes de fotos de latão não é uma palavra apr...       neg  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04078d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3d84c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine cada clichê estereotipado e exagerado de todos os filmes e programas de TV nas ruas de Brooklyn entre 1930 e 1980. Preencha-o com um elenco de caricaturas intercambiáveis \\u200b\\u200bem vez de personagens reais. Acrescente uma mistura de músicas de \"período\" e guitarras elétricas lamentáveis \\u200b\\u200bdurante as cenas \"estrondosas\". Em seguida, passe o tempo tentando descobrir ou importar qual dos Deuces vai ser morto no estrondo final anticlímax.Eu vou dar este filme aponta para não ser apenas mais uma comédia romântica, teen slasher, filme de ação explosivo, comédia de sexo adolescente, infantil musical, ou veículo de indicação ao Oscar. Mas trazer algo novo ou interessante para o gênero da tragédia das gangues de rua pode ter sido bom.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_pt'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'df_tokenized' será um dicionário com as keys ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "# 'input_ids' -> instâncias tokenizadas\n",
    "# 'token_type_ids' -> mascara usada em tarefas de classificação de frases em pares (será descartada nesta task)\n",
    "# 'attention_mask' -> mascara de atenção que destaca para o modelo os tokens de padding [PAD]\n",
    "df_tokenized = tokenizer.batch_encode_plus(\n",
    "    df['text_pt'], \n",
    "    return_tensors='pt', \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28dacb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_temp = df_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d84df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49441, 512]) torch.Size([49441, 512])\n"
     ]
    }
   ],
   "source": [
    "print(df_tokenized['input_ids'].shape, df_tokenized['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc20b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limajos\\AppData\\Local\\Temp\\ipykernel_18512\\45488885.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment'].apply(lambda x: 0 if (x == 'neg' or x=='negativo') else 1)\n"
     ]
    }
   ],
   "source": [
    "# [0, DATASET_LEN, MAX_LENGTH] = input_ids\n",
    "# [1, DATASET_LEN, MAX_LENGTH] = attention_mask\n",
    "X = torch.stack((df_tokenized['input_ids'], df_tokenized['attention_mask']), dim=0)\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: 0 if (x == 'neg' or x=='negativo') else 1)\n",
    "y = torch.Tensor(df['sentiment'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fe3aad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    24763\n",
       "1    24678\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe66baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.X = self.X.to(device)\n",
    "        self.y = y\n",
    "        self.y = self.y.to(device)\n",
    "        \n",
    "        self.len = len(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[:, idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60c49910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 34609, Val: 9888, Teste: 4944\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDataset(X, y)\n",
    "\n",
    "num_train_instances = np.int32(np.round(dataset.len * TRAIN_RATIO))\n",
    "num_val_instances = np.int32(np.round(dataset.len * VAL_RATIO))\n",
    "num_test_instances = np.int32(np.round(dataset.len * TEST_RATIO))\n",
    "print(f\"Treino: {num_train_instances}, Val: {num_val_instances}, Teste: {num_test_instances}\")\n",
    "\n",
    "train_split, val_split, test_split = torch.utils.data.random_split(dataset, [num_train_instances, num_val_instances, num_test_instances])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_split, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2171182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "epochs = 11#500\n",
    "steps_per_epochs = 200#200\n",
    "epochs_validation_samples = 50#50\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased').to(device)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "acc_calc = lambda output, labels : (labels == output.argmax(axis=1)).sum()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, 0.9997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f60973c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [59:22<00:00, 17.81s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:12:06<00:00, 86.52s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 1 \t Train loss: 0.5635520219802856 \t Train Acc: 0.6543750166893005 \t Val loss: 0.6135963797569275 \t Val acc: 0.706250011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:18<00:00, 17.49s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:00<00:00, 85.21s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 2 \t Train loss: 0.6375169157981873 \t Train Acc: 0.7149999737739563 \t Val loss: 0.6026898622512817 \t Val acc: 0.7200000286102295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:10<00:00, 17.45s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:10:43<00:00, 84.86s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 3 \t Train loss: 0.5701167583465576 \t Train Acc: 0.7384374737739563 \t Val loss: 0.4406938850879669 \t Val acc: 0.7437499761581421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:24<00:00, 17.52s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:04<00:00, 85.29s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 4 \t Train loss: 0.5418614745140076 \t Train Acc: 0.7515624761581421 \t Val loss: 0.5226828455924988 \t Val acc: 0.7662500143051147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:12<00:00, 17.46s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:10:50<00:00, 85.01s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 5 \t Train loss: 0.4114254415035248 \t Train Acc: 0.7549999952316284 \t Val loss: 0.3997673988342285 \t Val acc: 0.7837499976158142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:10<00:00, 17.45s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:09<00:00, 85.38s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 6 \t Train loss: 0.5844845175743103 \t Train Acc: 0.7584375143051147 \t Val loss: 0.44613105058670044 \t Val acc: 0.8062499761581421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:11<00:00, 17.46s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:10:50<00:00, 85.02s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 7 \t Train loss: 0.47239333391189575 \t Train Acc: 0.7690625190734863 \t Val loss: 0.4818926751613617 \t Val acc: 0.7950000166893005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [59:30<00:00, 17.85s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:12:21<00:00, 86.84s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 8 \t Train loss: 0.5010209083557129 \t Train Acc: 0.7793750166893005 \t Val loss: 0.48296597599983215 \t Val acc: 0.7987499833106995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:12<00:00, 17.46s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:10:49<00:00, 85.00s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 9 \t Train loss: 0.6366626024246216 \t Train Acc: 0.7696874737739563 \t Val loss: 0.48261117935180664 \t Val acc: 0.8050000071525574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:48<00:00, 17.64s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:17:59<00:00, 93.59s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 10 \t Train loss: 0.5461646914482117 \t Train Acc: 0.7909374833106995 \t Val loss: 0.3902184069156647 \t Val acc: 0.8025000095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:02:42<00:00, 18.81s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:15:15<00:00, 90.31s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 11 \t Train loss: 0.5236168503761292 \t Train Acc: 0.7706249952316284 \t Val loss: 0.48774901032447815 \t Val acc: 0.8025000095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_metadata = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    num_train_examples = 0\n",
    "    num_val_examples = 0\n",
    "    \n",
    "    train_hits = 0\n",
    "    val_hits = 0\n",
    "    \n",
    "    train_bar = tqdm(total=steps_per_epochs, desc=f\"Train\", unit=\"steps\", position=0, leave=True)\n",
    "    val_bar = tqdm(total=epochs_validation_samples, desc=f\"VAL\", unit=\"samples\", position=0, leave=True)\n",
    "    \n",
    "    for batch_number, (features, labels) in enumerate(train_loader):\n",
    "        \n",
    "        train_running_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        input_ids, input_masks = features[:, 0, :], features[:, 1, :]\n",
    "        \n",
    "        loss, logits = model(input_ids, input_masks, labels=labels.long()).to_tuple()\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        softmax_predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "        train_hits += acc_calc(softmax_predictions, labels)\n",
    "        \n",
    "        train_bar.update(1)\n",
    "        \n",
    "        num_train_examples += features.shape[0]\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (batch_number + 1) % steps_per_epochs == 0:\n",
    "            train_bar.close()\n",
    "            break\n",
    "            \n",
    "    for batch_number, (features, labels) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            val_running_loss = 0\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            input_ids, input_masks = features[:, 0, :], features[:, 1, :]\n",
    "            loss, logits = model(input_ids, input_masks, labels=labels.long()).to_tuple()\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            softmax_predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "            val_hits += acc_calc(softmax_predictions, labels)\n",
    "\n",
    "            num_val_examples += features.shape[0]\n",
    "\n",
    "            val_bar.update(1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if (batch_number + 1) % epochs_validation_samples == 0:\n",
    "                val_bar.close()\n",
    "                break\n",
    "    train_acc = torch.true_divide(train_hits, num_train_examples)\n",
    "    val_acc = torch.true_divide(val_hits, num_val_examples)\n",
    "    \n",
    "    print(f\"EPOCH SUMMARY - {i + 1} \\t Train loss: {train_running_loss} \\t Train Acc: {train_acc} \\t Val loss: {val_running_loss} \\t Val acc: {val_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec627e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:15:33<00:00, 22.67s/steps]\n",
      "VAL: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:34:04<00:00, 112.89s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 1 \t Train loss: 0.6464643478393555 \t Train Acc: 0.6621875166893005 \t Val loss: 0.6763045191764832 \t Val acc: 0.7074999809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:13:33<00:00, 22.07s/steps]\n",
      "VAL: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:27:23<00:00, 104.86s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 2 \t Train loss: 0.5170610547065735 \t Train Acc: 0.7212499976158142 \t Val loss: 0.49533841013908386 \t Val acc: 0.7487499713897705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:49<00:00, 17.65s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:29<00:00, 85.78s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 3 \t Train loss: 0.5603887438774109 \t Train Acc: 0.7487499713897705 \t Val loss: 0.5567473769187927 \t Val acc: 0.731249988079071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:44<00:00, 17.62s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:33<00:00, 85.87s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 4 \t Train loss: 0.587253987789154 \t Train Acc: 0.7574999928474426 \t Val loss: 0.6113306879997253 \t Val acc: 0.7762500047683716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:50<00:00, 17.65s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:46<00:00, 86.13s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 5 \t Train loss: 0.6173747777938843 \t Train Acc: 0.7521874904632568 \t Val loss: 0.5400698184967041 \t Val acc: 0.7574999928474426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:45<00:00, 17.63s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:36<00:00, 85.92s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 6 \t Train loss: 0.697488009929657 \t Train Acc: 0.7631250023841858 \t Val loss: 0.5169633030891418 \t Val acc: 0.8100000023841858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:30<00:00, 17.55s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:13<00:00, 85.47s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 7 \t Train loss: 0.3092273473739624 \t Train Acc: 0.7774999737739563 \t Val loss: 0.26815173029899597 \t Val acc: 0.7599999904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:41<00:00, 17.61s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:25<00:00, 85.72s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 8 \t Train loss: 0.4857879877090454 \t Train Acc: 0.765625 \t Val loss: 0.42343032360076904 \t Val acc: 0.768750011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [59:18<00:00, 17.79s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:12:10<00:00, 86.61s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 9 \t Train loss: 0.37913525104522705 \t Train Acc: 0.7790625095367432 \t Val loss: 0.43403375148773193 \t Val acc: 0.7987499833106995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:20<00:00, 17.50s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:04<00:00, 85.30s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 10 \t Train loss: 0.4495573043823242 \t Train Acc: 0.7671874761581421 \t Val loss: 0.624192476272583 \t Val acc: 0.7537500262260437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:24<00:00, 17.52s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:10<00:00, 85.41s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 11 \t Train loss: 0.6420261859893799 \t Train Acc: 0.7875000238418579 \t Val loss: 0.3692761957645416 \t Val acc: 0.8037499785423279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:02:02<00:00, 18.61s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:16:06<00:00, 91.33s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 12 \t Train loss: 0.5780278444290161 \t Train Acc: 0.7890625 \t Val loss: 0.3781621754169464 \t Val acc: 0.8075000047683716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:01:47<00:00, 18.54s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:15:41<00:00, 90.83s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 13 \t Train loss: 0.5607078075408936 \t Train Acc: 0.7815625071525574 \t Val loss: 0.3863183856010437 \t Val acc: 0.800000011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:16:53<00:00, 23.07s/steps]\n",
      "VAL: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:31:27<00:00, 109.76s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 14 \t Train loss: 0.42941415309906006 \t Train Acc: 0.7821875214576721 \t Val loss: 0.4362359642982483 \t Val acc: 0.8100000023841858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:03:55<00:00, 19.18s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:19:06<00:00, 94.93s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 15 \t Train loss: 0.45144084095954895 \t Train Acc: 0.7887499928474426 \t Val loss: 0.5177726745605469 \t Val acc: 0.8062499761581421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:06:40<00:00, 20.00s/steps]\n",
      "VAL: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:29:32<00:00, 107.44s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 16 \t Train loss: 0.45293670892715454 \t Train Acc: 0.7668750286102295 \t Val loss: 0.4699278473854065 \t Val acc: 0.8062499761581421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:28:54<00:00, 26.67s/steps]\n",
      "VAL: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:41:57<00:00, 122.34s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 17 \t Train loss: 0.5265647768974304 \t Train Acc: 0.7931249737739563 \t Val loss: 0.3290431499481201 \t Val acc: 0.8187500238418579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:19<00:00, 17.50s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:11:04<00:00, 85.29s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 18 \t Train loss: 0.5557351112365723 \t Train Acc: 0.7865625023841858 \t Val loss: 0.31768158078193665 \t Val acc: 0.8100000023841858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:07:26<00:00, 20.23s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:20:07<00:00, 96.14s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 19 \t Train loss: 0.6140297651290894 \t Train Acc: 0.7837499976158142 \t Val loss: 0.462960809469223 \t Val acc: 0.8050000071525574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:11<00:00, 17.46s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:10:47<00:00, 84.95s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 20 \t Train loss: 0.36306387186050415 \t Train Acc: 0.7850000262260437 \t Val loss: 0.3866851031780243 \t Val acc: 0.793749988079071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|████████████████████████████████████████████████████████████████████| 200/200 [1:00:17<00:00, 18.09s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:12:58<00:00, 87.57s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 21 \t Train loss: 0.4608862102031708 \t Train Acc: 0.7928125262260437 \t Val loss: 0.32558324933052063 \t Val acc: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [58:08<00:00, 17.44s/steps]\n",
      "VAL: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:10:48<00:00, 84.96s/samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 22 \t Train loss: 0.6076433658599854 \t Train Acc: 0.7853124737739563 \t Val loss: 0.45986542105674744 \t Val acc: 0.7912499904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  72%|██████████████████████████████████████████████████▍                   | 144/200 [42:07<21:21, 22.88s/steps]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     20\u001b[0m input_ids, input_masks \u001b[38;5;241m=\u001b[39m features[:, \u001b[38;5;241m0\u001b[39m, :], features[:, \u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m---> 22\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_tuple()\n\u001b[0;32m     24\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1854\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fe3c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"./modelimdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c331876",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_examples = 0\n",
    "\n",
    "test_hits = 0\n",
    "\n",
    "test_running_loss = 0\n",
    "\n",
    "for batch_number, (features, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            test_running_loss = 0\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            input_ids, input_masks = features[:, 0, :], features[:, 1, :]\n",
    "            loss, logits = model(input_ids, input_masks, labels=labels.long()).to_tuple()\n",
    "\n",
    "            test_running_loss += loss.item()\n",
    "\n",
    "            softmax_predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "            test_hits += acc_calc(softmax_predictions, labels)\n",
    "\n",
    "            num_test_examples += features.shape[0]\n",
    "            \n",
    "            # Get predicted label\n",
    "            _, predicted_label = torch.max(softmax_predictions, 1)\n",
    "\n",
    "            # Convert predicted_label tensor to a Python list\n",
    "            predicted_labels = predicted_label.tolist()\n",
    "\n",
    "            salvar_mostrar_resultado(labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9bdee27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8091)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = torch.true_divide(test_hits, num_test_examples)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5ce1b",
   "metadata": {},
   "source": [
    "# Avalidar modelo IMDB com a base dados ChatGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3128ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho dos arquivos salvos\n",
    "model_path = f\"./modelimdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5483032",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGpt = pd.read_excel(\"./dataset_concat_v3_.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03889c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positivo    159\n",
       "negativo    144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de valores válidos para a coluna 'sentiment'\n",
    "valores_validos = ['positivo', 'negativo']\n",
    "#valores_validos = ['pos', 'neg', 'positivo', 'negativo']\n",
    "\n",
    "# Filtrar o DataFrame mantendo apenas as linhas com valores válidos na coluna 'sentiment'\n",
    "df_filtrado = dfGpt[dfGpt['sentiment'].isin(valores_validos)]\n",
    "\n",
    "dfGpt = df_filtrado\n",
    "\n",
    "dfGpt['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0089fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo e o tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "# 'df_tokenized' será um dicionário com as keys ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "# 'input_ids' -> instâncias tokenizadas\n",
    "# 'token_type_ids' -> mascara usada em tarefas de classificação de frases em pares (será descartada nesta task)\n",
    "# 'attention_mask' -> mascara de atenção que destaca para o modelo os tokens de padding [PAD]\n",
    "dfGpt_tokenized = tokenizer.batch_encode_plus(\n",
    "    dfGpt['text_pt'], \n",
    "    return_tensors='pt', \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e07274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([303, 33]) torch.Size([303, 33])\n"
     ]
    }
   ],
   "source": [
    "print(dfGpt_tokenized['input_ids'].shape, dfGpt_tokenized['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5976686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0, DATASET_LEN, MAX_LENGTH] = input_ids\n",
    "# [1, DATASET_LEN, MAX_LENGTH] = attention_mask\n",
    "XGpt = torch.stack((dfGpt_tokenized['input_ids'], dfGpt_tokenized['attention_mask']), dim=0)\n",
    "\n",
    "dfGpt['sentiment'] = dfGpt['sentiment'].apply(lambda x: 0 if (x == 'neg' or x=='negativo') else 1)\n",
    "yGpt = torch.Tensor(dfGpt['sentiment'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d38c1aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    303\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfGpt['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dd18fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetGpt = TextDataset(XGpt, yGpt)\n",
    "\n",
    "testGpt_loader = torch.utils.data.DataLoader(datasetGpt, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15b9cf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t90\t213\t0\t0,00\t1,00\t0,297029702970297\t0,297029702970297\t0,5\t1,0\t1,0\t0,297029702970297\t0,1485148514851485\t0,297029702970297\t0,297029702970297\t0,297029702970297\t0,22900763358778625\t0,4580152671755725\t0,4580152671755725\t0,0\t0,7029702970297029\t0,7029702970297029\t1,0\n",
      "\n",
      "[[  0   0]\n",
      " [213  90]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      0.30      0.46       303\n",
      "\n",
      "    accuracy                           0.30       303\n",
      "   macro avg       0.50      0.15      0.23       303\n",
      "weighted avg       1.00      0.30      0.46       303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "num_test_examplesGpt = 0\n",
    "\n",
    "test_hitsGpt = 0\n",
    "\n",
    "test_running_lossGpt = 0\n",
    "\n",
    "for batch_numberGpt, (featuresGpt, labelsGpt) in enumerate(testGpt_loader):\n",
    "        with torch.no_grad():\n",
    "            test_running_lossGpt = 0\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            input_ids, input_masks = featuresGpt[:, 0, :], featuresGpt[:, 1, :]\n",
    "            loss, logits = model(input_ids, input_masks, labels=labelsGpt.long()).to_tuple()\n",
    "\n",
    "            test_running_lossGpt += loss.item()\n",
    "\n",
    "            acc_calcGpt = lambda output, labels : (labels == output.argmax(axis=1)).sum()\n",
    "        \n",
    "            softmax_predictionsGpt = torch.nn.functional.softmax(logits, dim=1)\n",
    "            test_hitsGpt += acc_calcGpt(softmax_predictionsGpt, labelsGpt)\n",
    "\n",
    "            num_test_examplesGpt += featuresGpt.shape[0]\n",
    "            \n",
    "            # Get predicted label\n",
    "            _, predicted_labelGpt = torch.max(softmax_predictionsGpt, 1)\n",
    "\n",
    "            # Convert predicted_label tensor to a Python list\n",
    "            predicted_labelsGpt = predicted_labelGpt.tolist()\n",
    "\n",
    "            salvar_mostrar_resultado(labelsGpt, predicted_labelsGpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ceceb4",
   "metadata": {},
   "source": [
    "# Gerar modelo com a base dados ChatGpt a partir do modelo IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a2948494",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGptModel = pd.read_excel(\"./dataset_concat_v3_.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d0ab6c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positivo    159\n",
       "negativo    144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de valores válidos para a coluna 'sentiment'\n",
    "valores_validos = ['positivo', 'negativo']\n",
    "#valores_validos = ['pos', 'neg', 'positivo', 'negativo']\n",
    "\n",
    "# Filtrar o DataFrame mantendo apenas as linhas com valores válidos na coluna 'sentiment'\n",
    "df_filtrado = dfGptModel[dfGptModel['sentiment'].isin(valores_validos)]\n",
    "\n",
    "dfGptModel = df_filtrado\n",
    "\n",
    "dfGptModel['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8811ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo e o tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "# 'df_tokenized' será um dicionário com as keys ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "# 'input_ids' -> instâncias tokenizadas\n",
    "# 'token_type_ids' -> mascara usada em tarefas de classificação de frases em pares (será descartada nesta task)\n",
    "# 'attention_mask' -> mascara de atenção que destaca para o modelo os tokens de padding [PAD]\n",
    "dfGptModel_tokenized = tokenizer.batch_encode_plus(\n",
    "    dfGptModel['text_pt'], \n",
    "    return_tensors='pt', \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f00cfdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([303, 33]) torch.Size([303, 33])\n"
     ]
    }
   ],
   "source": [
    "print(dfGptModel_tokenized['input_ids'].shape, dfGptModel_tokenized['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bbc0094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0, DATASET_LEN, MAX_LENGTH] = input_ids\n",
    "# [1, DATASET_LEN, MAX_LENGTH] = attention_mask\n",
    "XGptModel = torch.stack((dfGptModel_tokenized['input_ids'], dfGptModel_tokenized['attention_mask']), dim=0)\n",
    "\n",
    "dfGptModel['sentiment'] = dfGptModel['sentiment'].apply(lambda x: 0 if (x == 'neg' or x=='negativo') else 1)\n",
    "yGptModel = torch.Tensor(dfGptModel['sentiment'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "02b0ebde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    159\n",
       "0    144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfGptModel['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a649d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 212, Val: 61, Teste: 30\n"
     ]
    }
   ],
   "source": [
    "datasetGptModel = TextDataset(XGptModel, yGptModel)\n",
    "\n",
    "num_trainGptModel_instances = np.int32(np.round(datasetGptModel.len * TRAIN_RATIO))\n",
    "num_valGptModel_instances = np.int32(np.round(datasetGptModel.len * VAL_RATIO))\n",
    "num_testGptModel_instances = np.int32(np.round(datasetGptModel.len * TEST_RATIO))\n",
    "print(f\"Treino: {num_trainGptModel_instances}, Val: {num_valGptModel_instances}, Teste: {num_testGptModel_instances}\")\n",
    "\n",
    "trainGptModel_split, valGptModel_split, testGptModel_split = torch.utils.data.random_split(datasetGptModel, [num_trainGptModel_instances, num_valGptModel_instances, num_testGptModel_instances])\n",
    "\n",
    "trainGptModel_loader = torch.utils.data.DataLoader(trainGptModel_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valGptModel_loader = torch.utils.data.DataLoader(valGptModel_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#testGptModel_loader = torch.utils.data.DataLoader(testGptModel_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testGptModel_loader = torch.utils.data.DataLoader(testGptModel_split, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d934ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 11#500\n",
    "steps_per_epochs = 200#200\n",
    "epochs_validation_samples = 50#50\n",
    "\n",
    "# Caminho dos arquivos salvos\n",
    "model_path = f\"./modelimdb\"\n",
    "modelGptModel = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "\n",
    "for param in modelGptModel.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(modelGptModel.parameters())\n",
    "\n",
    "acc_calc = lambda output, labels : (labels == output.argmax(axis=1)).sum()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, 0.9997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1e18d986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▊                                                                | 14/200 [07:01<1:33:19, 30.11s/steps]\n",
      "VAL:   8%|█████▌                                                                | 4/50 [07:01<1:20:47, 105.37s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:08<00:58,  1.27s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 1 \t Train loss: 0.3187931180000305 \t Train Acc: 0.8396226167678833 \t Val loss: 0.4272170662879944 \t Val acc: 0.868852436542511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:08<01:46,  1.74steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:08<01:32,  2.01s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:08,  1.48s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 2 \t Train loss: 0.21408092975616455 \t Train Acc: 0.8301886916160583 \t Val loss: 0.45731469988822937 \t Val acc: 0.9344262480735779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:01,  1.54steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:44,  2.28s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:12<01:28,  1.91s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 3 \t Train loss: 0.34042030572891235 \t Train Acc: 0.8207547068595886 \t Val loss: 0.3201022148132324 \t Val acc: 0.9508196711540222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:12<02:39,  1.16steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:12<02:18,  3.01s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:10,  1.53s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 4 \t Train loss: 0.23435641825199127 \t Train Acc: 0.8962264060974121 \t Val loss: 0.2688409090042114 \t Val acc: 0.9508196711540222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:08,  1.45steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:51,  2.42s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:09,  1.50s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 5 \t Train loss: 0.4192183017730713 \t Train Acc: 0.8962264060974121 \t Val loss: 0.31596091389656067 \t Val acc: 0.9672130942344666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:05,  1.48steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:48,  2.36s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:09,  1.51s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 6 \t Train loss: 0.3524567484855652 \t Train Acc: 0.8773584961891174 \t Val loss: 0.2247443050146103 \t Val acc: 0.9672130942344666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:06,  1.47steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:49,  2.38s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:10,  1.54s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 7 \t Train loss: 0.4422670006752014 \t Train Acc: 0.8726415038108826 \t Val loss: 0.29709184169769287 \t Val acc: 0.9672130942344666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:08,  1.45steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:51,  2.42s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:09,  1.51s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 8 \t Train loss: 0.21307212114334106 \t Train Acc: 0.8726415038108826 \t Val loss: 0.19540023803710938 \t Val acc: 0.9672130942344666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:07,  1.46steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:50,  2.39s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:10,  1.53s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 9 \t Train loss: 0.22785943746566772 \t Train Acc: 0.9433962106704712 \t Val loss: 0.19945308566093445 \t Val acc: 0.9672130942344666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:07,  1.46steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:50,  2.40s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:11,  1.55s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 10 \t Train loss: 0.40130382776260376 \t Train Acc: 0.9103773832321167 \t Val loss: 0.22405889630317688 \t Val acc: 0.9672130942344666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   7%|████▉                                                                  | 14/200 [00:09<02:09,  1.44steps/s]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:09<01:51,  2.43s/samples]\n",
      "VAL:   8%|█████▊                                                                   | 4/50 [00:10<01:14,  1.63s/samples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH SUMMARY - 11 \t Train loss: 0.142496719956398 \t Train Acc: 0.900943398475647 \t Val loss: 0.1541433334350586 \t Val acc: 0.9672130942344666\n"
     ]
    }
   ],
   "source": [
    "epoch_metadata = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    num_train_examples = 0\n",
    "    num_val_examples = 0\n",
    "    \n",
    "    train_hits = 0\n",
    "    val_hits = 0\n",
    "    \n",
    "    train_bar = tqdm(total=steps_per_epochs, desc=f\"Train\", unit=\"steps\", position=0, leave=True)\n",
    "    val_bar = tqdm(total=epochs_validation_samples, desc=f\"VAL\", unit=\"samples\", position=0, leave=True)\n",
    "    \n",
    "    for batch_number, (features, labels) in enumerate(trainGptModel_loader):\n",
    "        \n",
    "        train_running_loss = 0\n",
    "        \n",
    "        modelGptModel.train()\n",
    "        \n",
    "        input_ids, input_masks = features[:, 0, :], features[:, 1, :]\n",
    "        \n",
    "        loss, logits = modelGptModel(input_ids, input_masks, labels=labels.long()).to_tuple()\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        softmax_predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "        train_hits += acc_calc(softmax_predictions, labels)\n",
    "        \n",
    "        train_bar.update(1)\n",
    "        \n",
    "        num_train_examples += features.shape[0]\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (batch_number + 1) % steps_per_epochs == 0:\n",
    "            train_bar.close()\n",
    "            break\n",
    "            \n",
    "    for batch_number, (features, labels) in enumerate(valGptModel_loader):\n",
    "        with torch.no_grad():\n",
    "            val_running_loss = 0\n",
    "\n",
    "            modelGptModel.eval()\n",
    "\n",
    "            input_ids, input_masks = features[:, 0, :], features[:, 1, :]\n",
    "            loss, logits = modelGptModel(input_ids, input_masks, labels=labels.long()).to_tuple()\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            softmax_predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "            val_hits += acc_calc(softmax_predictions, labels)\n",
    "\n",
    "            num_val_examples += features.shape[0]\n",
    "\n",
    "            val_bar.update(1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if (batch_number + 1) % epochs_validation_samples == 0:\n",
    "                val_bar.close()\n",
    "                break\n",
    "    train_acc = torch.true_divide(train_hits, num_train_examples)\n",
    "    val_acc = torch.true_divide(val_hits, num_val_examples)\n",
    "    \n",
    "    print(f\"EPOCH SUMMARY - {i + 1} \\t Train loss: {train_running_loss} \\t Train Acc: {train_acc} \\t Val loss: {val_running_loss} \\t Val acc: {val_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1c5760fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelGptModel.save_pretrained(f\"./modelimdb_chatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "918b3b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\t14\t1\t1\t0,93\t0,93\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,9333333333333333\t0,8666666666666667\t0,06666666666666667\t0,06666666666666667\t0,0\ttensor([0,, 1,, 0,, 1,, 1,, 0,, 1,, 0,, 1,, 1,, 0,, 1,, 1,, 1,, 0,, 1,, 0,, 1,,\n",
      "        0,, 0,, 1,, 0,, 1,, 0,, 0,, 1,, 0,, 1,, 0,, 0,])\t[0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "\n",
      "[[14  1]\n",
      " [ 1 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.93      0.93        15\n",
      "         1.0       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_examples = 0\n",
    "\n",
    "test_hits = 0\n",
    "\n",
    "test_running_loss = 0\n",
    "\n",
    "all_input_ids = []\n",
    "all_labels = []\n",
    "\n",
    "for batch_number, (features, labels) in enumerate(testGptModel_loader):\n",
    "        with torch.no_grad():\n",
    "            test_running_loss = 0\n",
    "\n",
    "            modelGptModel.eval()\n",
    "\n",
    "            input_ids, input_masks = features[:, 0, :], features[:, 1, :]\n",
    "            loss, logits = modelGptModel(input_ids, input_masks, labels=labels.long()).to_tuple()\n",
    "\n",
    "            all_input_ids += input_ids\n",
    "            all_labels += labels\n",
    "            \n",
    "            test_running_loss += loss.item()\n",
    "\n",
    "            softmax_predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "            test_hits += acc_calc(softmax_predictions, labels)\n",
    "\n",
    "            num_test_examples += features.shape[0]\n",
    "            \n",
    "            # Get predicted label\n",
    "            _, predicted_label = torch.max(softmax_predictions, 1)\n",
    "\n",
    "            # Convert predicted_label tensor to a Python list\n",
    "            predicted_labels = predicted_label.tolist()\n",
    "\n",
    "            salvar_mostrar_resultado(labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8754b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "train_test = torch.true_divide(test_hits, num_test_examples)\n",
    "print(test_hits)\n",
    "print(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "47071893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Texto:  Excel ##ente produto , fácil de usar e muito eficaz na refrig ##eração .\n",
      "1\n",
      "Texto:  Não recomend ##o , apresentou defe ##itos logo nos primeiros dias .\n",
      "2\n",
      "Texto:  Rec ##ome ##ndo fortemente este ar - condicion ##ado , sua eficiência e facilidade de operação são impressionante ##s .\n",
      "3\n",
      "Texto:  Rui ##m , bar ##ul ##hen ##to e não res ##fri ##a o ambiente adequada ##mente .\n",
      "4\n",
      "Texto:  Nenhum problema , sem falhas !\n",
      "5\n",
      "Texto:  Est ##ou satisf ##eito com a eficiência e economia deste produto .\n",
      "6\n",
      "Texto:  Sem nenhum defe ##ito , excelente !\n",
      "7\n",
      "Texto:  Re ##fri ##ger ##a bem , mas é bar ##ul ##hen ##to .\n",
      "8\n",
      "Texto:  Ad ##qui ##ri recentemente o ar - condicion ##ado Sam ##su ##ng e estou extremamente satisf ##eito com sua capacidade de refrig ##eração eficiente e economia de energia .\n",
      "9\n",
      "Texto:  Infelizmente , este ar - condicion ##ado não atende ##u às expectativas , apresentando falhas frequentes e preocupa ##ntes .\n",
      "10\n",
      "Texto:  Est ##ou muito conte ##nte com a compra deste ar - condicion ##ado Sam ##su ##ng , sua pratic ##idade e alto desempenho são excelentes .\n",
      "11\n",
      "Texto:  E ##ficiente na refrig ##eração , estou satisf ##eito .\n",
      "12\n",
      "Texto:  Rec ##ome ##ndo amplamente este ar - condicion ##ado Sam ##su ##ng , sua eficiência e facilidade de uso são impressionante ##s .\n",
      "13\n",
      "Texto:  Def ##ini ##tivamente , não recomend ##o este produto devido aos inúmeros problemas apresentados , um verdadeiro desper ##d ##ício de dinheiro .\n",
      "14\n",
      "Texto:  Encont ##rei muitos problemas , não foi bom .\n",
      "15\n",
      "Texto:  Nada funcionou como deveria , péss ##ima experiência .\n",
      "16\n",
      "Texto:  Não esperava tantos problemas com este ar - condicion ##ado Sam ##su ##ng , desde o início foram falhas constantes e insatisf ##atórias .\n",
      "17\n",
      "Texto:  Não atende ##u minha ##s expectativas , ar - condicion ##ado ruim .\n",
      "18\n",
      "Texto:  Não atende ##u às expectativas , muitos problemas .\n",
      "19\n",
      "Texto:  Não esperava tantos contra ##tempo ##s com este ar - condicion ##ado Sam ##su ##ng , desde o início foram falhas frequentes .\n",
      "20\n",
      "Texto:  Nada de errado , serviço excelente !\n",
      "21\n",
      "Texto:  Deixou muito a deseja ##r , várias falhas .\n",
      "22\n",
      "Texto:  Est ##ou bastante impression ##ado com a capacidade de res ##fri ##amento deste ar - condicion ##ado , é excepcional .\n",
      "23\n",
      "Texto:  Est ##ou verdadeiramente impression ##ado com a qualidade deste ar - condicion ##ado Sam ##su ##ng , sua eficiência é excepcional .\n",
      "24\n",
      "Texto:  Fi ##quei decep ##cionado com a qualidade deste ar - condicion ##ado .\n",
      "25\n",
      "Texto:  Não recomend ##o este ar - condicion ##ado Sam ##su ##ng , desde o início apresentou problemas constantes .\n",
      "26\n",
      "Texto:  Est ##ou extremamente satisf ##eito com a compra deste ar - condicion ##ado Sam ##su ##ng , sua pratic ##idade e alto desempenho são surpreendente ##s .\n",
      "27\n",
      "Texto:  Não encont ##rei nenhum defe ##ito , recomend ##o totalmente .\n",
      "28\n",
      "Texto:  Infelizmente , o desempenho deste ar - condicion ##ado deixou muito a deseja ##r , não atendendo às expectativas .\n",
      "29\n",
      "Texto:  Est ##ou impression ##ado com a qualidade deste ar - condicion ##ado Sam ##su ##ng , seu desempenho excepcional superou minha ##s expectativas .\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(all_input_ids):\n",
    "    print(index)\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(item)\n",
    "    # Remove special tokens and convert to string\n",
    "    predicted_tokens = [token for token in predicted_tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    predicted_text = ' '.join(predicted_tokens)\n",
    "    print('Texto: ', predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6613b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(0.)]\n"
     ]
    }
   ],
   "source": [
    "allLabels = []\n",
    "for batch_number, (features, labels) in enumerate(testGptModel_loader):\n",
    "    allLabels += labels\n",
    "print(allLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61d31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = testGptModel_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "88ab4882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Texto:  Fi ##quei desa ##pon ##tado , muitas coisas er ##radas .\n",
      "1\n",
      "Texto:  Est ##ou sur ##pres ##o com a capacidade de refrig ##eração deste ar - condicion ##ado Sam ##su ##ng , é altamente eficiente e econômico .\n",
      "2\n",
      "Texto:  Dec ##ep ##cionado com a qualidade deste ar - condicion ##ado Sam ##su ##ng , desde o início apresentou defe ##itos constantes .\n",
      "3\n",
      "Texto:  Não encont ##rei nenhuma falha , tudo perfeito .\n",
      "4\n",
      "Texto:  Não encont ##rei nenhum defe ##ito , recomend ##o totalmente .\n",
      "5\n",
      "Texto:  Deixou a deseja ##r , não atende ##u minha ##s expectativas .\n",
      "6\n",
      "Texto:  Est ##ou extremamente satisf ##eito com a qualidade deste ar - condicion ##ado Sam ##su ##ng , sua eficiência e desempenho são excelentes .\n",
      "7\n",
      "Texto:  Não recomend ##o este ar - condicion ##ado , sua baixa dura ##bilidade e eficiência abaixo do esperado são preocupa ##ntes .\n",
      "8\n",
      "Texto:  Est ##ou satisf ##eito com a eficiência deste produto .\n",
      "9\n",
      "Texto:  Desem ##penh ##o satisfa ##tório , estou satisf ##eito .\n",
      "10\n",
      "Texto:  Não corresponde ao anunciado , não recomend ##o .\n",
      "11\n",
      "Texto:  Desa ##pon ##tado com a dura ##bilidade deste produto , os problemas começaram logo após o período de garantia .\n",
      "12\n",
      "Texto:  Ó ##timo desempenho , eficiente e econômico .\n",
      "13\n",
      "Texto:  Est ##ou sur ##pres ##o com a capacidade de refrig ##eração deste ar - condicion ##ado , é muito eficiente e econômico .\n",
      "14\n",
      "Texto:  Não atende ##u às expectativas , muitos problemas .\n",
      "15\n",
      "Texto:  Não recomend ##o , apresentou falhas desde o início .\n",
      "16\n",
      "Texto:  Não recomend ##o este ar - condicion ##ado Sam ##su ##ng , sua capacidade de refrig ##eração é abaixo do esperado e seu consumo de energia é alto .\n",
      "17\n",
      "Texto:  Infelizmente , este ar - condicion ##ado Sam ##su ##ng não atende ##u às expectativas , sua capacidade de res ##fri ##amento é bastante limitada .\n",
      "18\n",
      "Texto:  Não houve nenhum problema detec ##tado , muito satisf ##eito .\n",
      "19\n",
      "Texto:  O desempenho deste ar - condicion ##ado Sam ##su ##ng superou minha ##s expectativas , estou extremamente satisf ##eito com sua eficiência .\n",
      "20\n",
      "Texto:  Não recomend ##o , apresentou defe ##itos desde o início .\n",
      "21\n",
      "Texto:  Infelizmente , este ar - condicion ##ado Sam ##su ##ng não corresponde ##u às expectativas , seu desempenho deixou muito a deseja ##r .\n",
      "22\n",
      "Texto:  Não recomend ##o , apresentou problemas frequentes .\n",
      "23\n",
      "Texto:  Infelizmente , o desempenho deste ar - condicion ##ado Sam ##su ##ng deixou muito a deseja ##r , não atendendo às expectativas .\n",
      "24\n",
      "Texto:  Re ##fri ##ger ##a bem , mas faz muito bar ##ulho .\n",
      "25\n",
      "Texto:  Não recomend ##o este ar - condicion ##ado Sam ##su ##ng , desde o início apresentou problemas constantes .\n",
      "26\n",
      "Texto:  E ##ficiente e de qualidade , estou muito satisf ##eito .\n",
      "27\n",
      "Texto:  Est ##ou completamente satisf ##eito com este ar - condicion ##ado Sam ##su ##ng , sua eficiência e desempenho superar ##am minha ##s expectativas .\n",
      "28\n",
      "Texto:  Este ar - condicion ##ado não cump ##re o que promete .\n",
      "29\n",
      "Texto:  Não vi nenhum defe ##ito .\n"
     ]
    }
   ],
   "source": [
    "#predicted_tokens = tokenizer.convert_ids_to_tokens(list(data)[0][1])\n",
    "#print(data[0][0][0])\n",
    "#for t in list(data):\n",
    "for index, item in enumerate(data):\n",
    "    print(index)\n",
    "    #print(t[0][0])\n",
    "    # Convert token IDs back to tokens\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(item[0][0])\n",
    "    # Remove special tokens and convert to string\n",
    "    predicted_tokens = [token for token in predicted_tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    predicted_text = ' '.join(predicted_tokens)\n",
    "    print('Texto: ', predicted_text)\n",
    "    #print('index: ', tokenizer.convert_ids_to_tokens(item[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ccf80c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Texto:  Fi ##quei desa ##pon ##tado , muitas coisas er ##radas .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "1\n",
      "Texto:  Est ##ou sur ##pres ##o com a capacidade de refrig ##eração deste ar - condicion ##ado Sam ##su ##ng , é altamente eficiente e econômico .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "2\n",
      "Texto:  Dec ##ep ##cionado com a qualidade deste ar - condicion ##ado Sam ##su ##ng , desde o início apresentou defe ##itos constantes .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "3\n",
      "Texto:  Não encont ##rei nenhuma falha , tudo perfeito .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "4\n",
      "Texto:  Não encont ##rei nenhum defe ##ito , recomend ##o totalmente .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "5\n",
      "Texto:  Deixou a deseja ##r , não atende ##u minha ##s expectativas .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "6\n",
      "Texto:  Est ##ou extremamente satisf ##eito com a qualidade deste ar - condicion ##ado Sam ##su ##ng , sua eficiência e desempenho são excelentes .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "7\n",
      "Texto:  Não recomend ##o este ar - condicion ##ado , sua baixa dura ##bilidade e eficiência abaixo do esperado são preocupa ##ntes .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "8\n",
      "Texto:  Est ##ou satisf ##eito com a eficiência deste produto .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "9\n",
      "Texto:  Desem ##penh ##o satisfa ##tório , estou satisf ##eito .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "10\n",
      "Texto:  Não corresponde ao anunciado , não recomend ##o .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "11\n",
      "Texto:  Desa ##pon ##tado com a dura ##bilidade deste produto , os problemas começaram logo após o período de garantia .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "12\n",
      "Texto:  Ó ##timo desempenho , eficiente e econômico .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "13\n",
      "Texto:  Est ##ou sur ##pres ##o com a capacidade de refrig ##eração deste ar - condicion ##ado , é muito eficiente e econômico .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "14\n",
      "Texto:  Não atende ##u às expectativas , muitos problemas .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "15\n",
      "Texto:  Não recomend ##o , apresentou falhas desde o início .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "16\n",
      "Texto:  Não recomend ##o este ar - condicion ##ado Sam ##su ##ng , sua capacidade de refrig ##eração é abaixo do esperado e seu consumo de energia é alto .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
      "17\n",
      "Texto:  Infelizmente , este ar - condicion ##ado Sam ##su ##ng não atende ##u às expectativas , sua capacidade de res ##fri ##amento é bastante limitada .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "18\n",
      "Texto:  Não houve nenhum problema detec ##tado , muito satisf ##eito .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "19\n",
      "Texto:  O desempenho deste ar - condicion ##ado Sam ##su ##ng superou minha ##s expectativas , estou extremamente satisf ##eito com sua eficiência .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "20\n",
      "Texto:  Não recomend ##o , apresentou defe ##itos desde o início .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "21\n",
      "Texto:  Infelizmente , este ar - condicion ##ado Sam ##su ##ng não corresponde ##u às expectativas , seu desempenho deixou muito a deseja ##r .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "22\n",
      "Texto:  Não recomend ##o , apresentou problemas frequentes .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "23\n",
      "Texto:  Infelizmente , o desempenho deste ar - condicion ##ado Sam ##su ##ng deixou muito a deseja ##r , não atendendo às expectativas .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "24\n",
      "Texto:  Re ##fri ##ger ##a bem , mas faz muito bar ##ulho .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "25\n",
      "Texto:  Não recomend ##o este ar - condicion ##ado Sam ##su ##ng , desde o início apresentou problemas constantes .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "26\n",
      "Texto:  E ##ficiente e de qualidade , estou muito satisf ##eito .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "27\n",
      "Texto:  Est ##ou completamente satisf ##eito com este ar - condicion ##ado Sam ##su ##ng , sua eficiência e desempenho superar ##am minha ##s expectativas .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "28\n",
      "Texto:  Este ar - condicion ##ado não cump ##re o que promete .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "29\n",
      "Texto:  Não vi nenhum defe ##ito .\n",
      "Label:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#predicted_tokens = tokenizer.convert_ids_to_tokens(list(data)[0][1])\n",
    "#print(data[0][0][0])\n",
    "#for t in list(data):\n",
    "for index, item in enumerate(data):\n",
    "    print(index)\n",
    "    #print(t[0][0])\n",
    "    # Convert token IDs back to tokens\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(item[0][0])\n",
    "    # Remove special tokens and convert to string\n",
    "    predicted_tokens = [token for token in predicted_tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    predicted_text = ' '.join(predicted_tokens)\n",
    "    print('Texto: ', predicted_text)\n",
    "    #print(tokenizer.convert_ids_to_tokens(t[0][0]))\n",
    "    print('Label: ', item[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d96620f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[  101, 19687, 19623,  1950,  1108,   487,   117,  1615,  4486,  2317,\n",
       "            2208,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  9009,   203,  1401,   491, 22280,   170,   123,  2839,   125,\n",
       "           16373,  3917,  2166,   388,   118, 13775,   201,  3021,  2515,   833,\n",
       "             117,   253,  7532, 10746,   122,  5912,   119,   102,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  4534,  7826,  8970,   170,   123,  3322,  2166,   388,   118,\n",
       "           13775,   201,  3021,  2515,   833,   117,  1065,   146,  1206,  3794,\n",
       "            2455,   721,  9886,   119,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  2542,   778,  8393,  3963, 10216,   117,  2745, 13380,   119,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  2542,   778,  8393,  3484,  2455,   373,   117,  9099, 22280,\n",
       "            4171,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101, 19492,   123,  8781, 22282,   117,   346, 16761, 22288,  7122,\n",
       "           22281, 15686,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  9009,   203,  5762,  8174,   788,   170,   123,  3322,  2166,\n",
       "             388,   118, 13775,   201,  3021,  2515,   833,   117,   327, 11446,\n",
       "             122,  4576,   453, 19710,   119,   102,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  2542,  9099, 22280,   860,   388,   118, 13775,   201,   117,\n",
       "             327,  4098,  3072,  1758,   122, 11446,  4133,   171,  9873,   453,\n",
       "            5811,   358,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  9009,   203,  8174,   788,   170,   123, 11446,  2166,  3576,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101, 21680,  7350, 22280, 12458,  1251,   117, 12044,  8174,   788,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  2542,  8059,   320,  4775,   117,   346,  9099, 22280,   119,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101, 18927,  1108,   487,   170,   123,  3072,  1758,  2166,  3576,\n",
       "             117,   259,  2394,  3398,  2044,   790,   146,  1254,   125, 13649,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  6679,  3608,  4576,   117, 10746,   122,  5912,   119,   102,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  9009,   203,  1401,   491, 22280,   170,   123,  2839,   125,\n",
       "           16373,  3917,  2166,   388,   118, 13775,   201,   117,   253,   785,\n",
       "           10746,   122,  5912,   119,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  2542, 16761, 22288,  1000, 15686,   117,  1415,  2394,   119,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  2542,  9099, 22280,   117,  3794, 12365,  1065,   146,  1206,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  2542,  9099, 22280,   860,   388,   118, 13775,   201,  3021,\n",
       "            2515,   833,   117,   327,  2839,   125, 16373,  3917,   253,  4133,\n",
       "             171,  9873,   122,   347,  6309,   125,  2608,   253,  2979,   119,\n",
       "             102,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101, 17303,   117,   860,   388,   118, 13775,   201,  3021,  2515,\n",
       "             833,   346, 16761, 22288,  1000, 15686,   117,   327,  2839,   125,\n",
       "             398, 12886,  1845,   253,  2780,  8946,   119,   102,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[ 101, 2542, 3363, 3484, 3350, 8070,  487,  117,  785, 8174,  788,  119,\n",
       "            102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "          [   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "              1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,   231,  4576,  2166,   388,   118, 13775,   201,  3021,  2515,\n",
       "             833, 20110,  7122, 22281, 15686,   117, 12044,  5762,  8174,   788,\n",
       "             170,   327, 11446,   119,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  2542,  9099, 22280,   117,  3794,  2455,   721,  1065,   146,\n",
       "            1206,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101, 17303,   117,   860,   388,   118, 13775,   201,  3021,  2515,\n",
       "             833,   346,  8059, 22288,  1000, 15686,   117,   347,  4576,  2789,\n",
       "             785,   123,  8781, 22282,   119,   102,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  2542,  9099, 22280,   117,  3794,  2394, 11845,   119,   102,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101, 17303,   117,   146,  4576,  2166,   388,   118, 13775,   201,\n",
       "            3021,  2515,   833,  2789,   785,   123,  8781, 22282,   117,   346,\n",
       "           20431,  1000, 15686,   119,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,   482, 12886,  2324, 22278,  1004,   117,   449,   659,   785,\n",
       "            1923,  1259,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,  2542,  9099, 22280,   860,   388,   118, 13775,   201,  3021,\n",
       "            2515,   833,   117,  1065,   146,  1206,  3794,  2394,  9886,   119,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[  101,   192, 11869,   122,   125,  3322,   117, 12044,   785,  8174,\n",
       "             788,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  9009,   203,  4304,  8174,   788,   170,   860,   388,   118,\n",
       "           13775,   201,  3021,  2515,   833,   117,   327, 11446,   122,  4576,\n",
       "           12929,   228,  7122, 22281, 15686,   119,   102,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(1.)),\n",
       " (tensor([[  101,  1681,   388,   118, 13775,   201,   346,  4395,   130,   146,\n",
       "             179, 19790,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0],\n",
       "          [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0]]),\n",
       "  tensor(0.)),\n",
       " (tensor([[ 101, 2542, 1976, 3484, 2455,  373,  119,  102,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "          [   1,    1,    1,    1,    1,    1,    1,    1,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       "  tensor(1.))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9288a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.), tensor(0.), tensor(1.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.)]\n"
     ]
    }
   ],
   "source": [
    "allLabels = []\n",
    "for batch_number, (features, labels) in enumerate(testGptModel_loader):\n",
    "    allLabels += labels\n",
    "print(allLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dd0f023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2685001f7d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testGptModel_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d502a969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2685001f7d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testGptModel_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "402c3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2, 33])\n"
     ]
    }
   ],
   "source": [
    "i1, l1 = next(iter(testGptModel_loader))\n",
    "print(i1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "14762c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f54df2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4525d41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'to_excel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtestGptModel_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtestGptModel_loader.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlsxwriter\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'to_excel'"
     ]
    }
   ],
   "source": [
    "testGptModel_loader.to_excel(r'testGptModel_loader.xlsx', engine='xlsxwriter', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a25e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379b31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf517a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee605c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cecf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009f554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf966c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca59273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4afca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset_concat_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28e1a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50038</th>\n",
       "      <td>50055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experiência excelente, sem nenhum defeito!</td>\n",
       "      <td>positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50039</th>\n",
       "      <td>50056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Não gostei, muitos problemas encontrados.</td>\n",
       "      <td>negativo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50040</th>\n",
       "      <td>50057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sem problemas detectados, serviço perfeito!</td>\n",
       "      <td>positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50041</th>\n",
       "      <td>50058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deixou muito a desejar, não recomendo.</td>\n",
       "      <td>negativo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50042</th>\n",
       "      <td>50059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serviço excelente, sem nenhum defeito!</td>\n",
       "      <td>positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50043 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            text_en  \\\n",
       "0          1  Once again Mr. Costner has dragged out a movie...   \n",
       "1          2  This is an example of why the majority of acti...   \n",
       "2          3  First of all I hate those moronic rappers, who...   \n",
       "3          4  Not even the Beatles could write songs everyon...   \n",
       "4          5  Brass pictures movies is not a fitting word fo...   \n",
       "...      ...                                                ...   \n",
       "50038  50055                                                NaN   \n",
       "50039  50056                                                NaN   \n",
       "50040  50057                                                NaN   \n",
       "50041  50058                                                NaN   \n",
       "50042  50059                                                NaN   \n",
       "\n",
       "                                                 text_pt sentiment Unnamed: 4  \\\n",
       "0      Mais uma vez, o Sr. Costner arrumou um filme p...       neg        NaN   \n",
       "1      Este é um exemplo do motivo pelo qual a maiori...       neg        NaN   \n",
       "2      Primeiro de tudo eu odeio esses raps imbecis, ...       neg        NaN   \n",
       "3      Nem mesmo os Beatles puderam escrever músicas ...       neg        NaN   \n",
       "4      Filmes de fotos de latão não é uma palavra apr...       neg        NaN   \n",
       "...                                                  ...       ...        ...   \n",
       "50038         Experiência excelente, sem nenhum defeito!  positivo        NaN   \n",
       "50039          Não gostei, muitos problemas encontrados.  negativo        NaN   \n",
       "50040        Sem problemas detectados, serviço perfeito!  positivo        NaN   \n",
       "50041             Deixou muito a desejar, não recomendo.  negativo        NaN   \n",
       "50042             Serviço excelente, sem nenhum defeito!  positivo        NaN   \n",
       "\n",
       "      Unnamed: 5  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "...          ...  \n",
       "50038        NaN  \n",
       "50039        NaN  \n",
       "50040        NaN  \n",
       "50041        NaN  \n",
       "50042        NaN  \n",
       "\n",
       "[50043 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313f0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2474ba20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50038</th>\n",
       "      <td>50055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experiência excelente, sem nenhum defeito!</td>\n",
       "      <td>positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50039</th>\n",
       "      <td>50056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Não gostei, muitos problemas encontrados.</td>\n",
       "      <td>negativo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50040</th>\n",
       "      <td>50057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sem problemas detectados, serviço perfeito!</td>\n",
       "      <td>positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50041</th>\n",
       "      <td>50058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deixou muito a desejar, não recomendo.</td>\n",
       "      <td>negativo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50042</th>\n",
       "      <td>50059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serviço excelente, sem nenhum defeito!</td>\n",
       "      <td>positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50043 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            text_en  \\\n",
       "0          1  Once again Mr. Costner has dragged out a movie...   \n",
       "1          2  This is an example of why the majority of acti...   \n",
       "2          3  First of all I hate those moronic rappers, who...   \n",
       "3          4  Not even the Beatles could write songs everyon...   \n",
       "4          5  Brass pictures movies is not a fitting word fo...   \n",
       "...      ...                                                ...   \n",
       "50038  50055                                                NaN   \n",
       "50039  50056                                                NaN   \n",
       "50040  50057                                                NaN   \n",
       "50041  50058                                                NaN   \n",
       "50042  50059                                                NaN   \n",
       "\n",
       "                                                 text_pt sentiment Unnamed: 4  \\\n",
       "0      Mais uma vez, o Sr. Costner arrumou um filme p...       neg        NaN   \n",
       "1      Este é um exemplo do motivo pelo qual a maiori...       neg        NaN   \n",
       "2      Primeiro de tudo eu odeio esses raps imbecis, ...       neg        NaN   \n",
       "3      Nem mesmo os Beatles puderam escrever músicas ...       neg        NaN   \n",
       "4      Filmes de fotos de latão não é uma palavra apr...       neg        NaN   \n",
       "...                                                  ...       ...        ...   \n",
       "50038         Experiência excelente, sem nenhum defeito!  positivo        NaN   \n",
       "50039          Não gostei, muitos problemas encontrados.  negativo        NaN   \n",
       "50040        Sem problemas detectados, serviço perfeito!  positivo        NaN   \n",
       "50041             Deixou muito a desejar, não recomendo.  negativo        NaN   \n",
       "50042             Serviço excelente, sem nenhum defeito!  positivo        NaN   \n",
       "\n",
       "      Unnamed: 5  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "...          ...  \n",
       "50038        NaN  \n",
       "50039        NaN  \n",
       "50040        NaN  \n",
       "50041        NaN  \n",
       "50042        NaN  \n",
       "\n",
       "[50043 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "115357e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'dataset_concat_v3.xlsx', engine='xlsxwriter', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05f4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
